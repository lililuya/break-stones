超参数：人为指定的参数

批量大小和学习率。

先对函数进行sum操作，在求backward，那么我们再进行 l.sum() 的相加操作时得到的就是 [f1(w,b)+f2(w,b)+f3(w,b)+f4(w,b)+f5(w,b)+f6(w,b)+f7(w,b)+f8(w,b)+f9(w,b)+f10(w,b)] 这样的一个标量，对他就可以使用 .backward() 求梯度（也就是对于各个f(w,b)式子求导再加起来），最后得到的是各个梯度的和。



归回和分类区别

![image-20240721193407464](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/EnumDemo202407211934632.png)

softmax使用交叉熵



由于计算机存储的数值有上线，为了保证稳定性，使用了LogSumExp， LogSumExp则是exp-normalize的归一化的思想。具体可以参考链接https://blog.csdn.net/yjw123456/article/details/121869249







激活函数：

sigmoid(0~1)



tanh(-1~1)



ReLu(x)=max(x,0)

---

测试数据集分割

k-则交叉验证



过拟合和欠拟合：根据数据的复杂程度选择模型的容量

*训练误差*（training error）是指， 模型在训练数据集上计算得到的误差。 *泛化误差*（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。



正则化：凡是可以减少泛化误差而不是减少训练误差的方法都可以称作是正则化方法



权重衰退：要保证权重向量比较小， 最常用方法是将其范数作为惩罚项加到最小化损失的问题中。 将原来的训练目标*最小化训练标签上的预测损失*， 调整为*最小化预测损失和惩罚项之和*。（权重衰退我觉得有点像之前学习过的baseLine的效果，通过引入一个值，不至于让他的曲线过于极端也就是权重值过大？）

丢弃法：通常将丢弃法作用在隐藏全连接层的输出上（正则只在训练中使用）

做乘法比去选择一个函数来得快



BN是作用在卷积层用的

*前向传播*（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。

*反向传播*（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 简言之，该方法根据微积分中的*链式规则*，按相反的顺序从输出层到输入层遍历网络。 该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。



Xavier初始化（也被称为Glorot初始化，以其发明者Xavier Glorot命名）是一种用于初始化神经网络权重的方法，特别是在训练深度神经网络时。这种初始化方法在启动训练时有助于保持输入和输出的方差一致，从而避免在网络的深层中发生梯度消失或梯度爆炸的问题。





对每一次参数初始化的时候，不能把weight初始化成常数，会导致对称性问题，用dropout正则化可以打破

![image-20240724141754199](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/EnumDemo202407241418276.png)



 共享权重的操作

![image-20240724142257313](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/EnumDemo202407241422402.png)

保存和加载张量

torch.save 和torch.load

![image-20240724143309463](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/EnumDemo202407241433524.png)

存储张量列表和张量字典

![image-20240724143452928](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/EnumDemo202407241434000.png)



---

二维图像具有的性质：

### 平移不变性

平移不变性。 这意味着检测对象在输入𝑋中的平移，应该仅导致隐藏表示𝐻中的平移。也就是说，𝑉和𝑈实际上不依赖于(𝑖,𝑗)的值，即[𝑉]𝑖,𝑗,𝑎,𝑏=[𝑉]𝑎,𝑏。并且𝑈是一个常数，比如𝑢。

![image-20240725083822248](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/EnumDemo202407250838302.png)

### 局部性

局部性。如上所述，为了收集用来训练参数[𝐻]𝑖,𝑗的相关信息，我们不应偏离到距(𝑖,𝑗)很远的地方。这意味着在|𝑎|>Δ或|𝑏|>Δ的范围之外，我们可以设置[𝑉]𝑎,𝑏=0

![image-20240725083838579](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/EnumDemo202407250838614.png)



卷积的填充操作

通过一定的技巧对P取值，可以使得图片的大小保持不变，这样可以保证无论多少层的神经网络，都可以进行相应的操作（如果每次一都把图片变小，会导致后续的输入数据不足以进行卷积等相关操作）

![image-20240725090140303](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/EnumDemo202407250901461.png)

步幅的操作



![image-20240725090658373](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/EnumDemo202407250906581.png)



步幅可以成倍的缩小图片大小

![image-20240725091325695](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407250913814.png)

填充和步幅小结

![image-20240725091757250](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407250917409.png)

他这个地方的padding应该是等于(kernel_size-1)/2

![image-20240725092213822](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407250922937.png)

用多个3x3的卷积比5x5的相同效果训练快一点

二维卷积公式

![image-20240725102509188](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407251025346.png)

输出通道是当前层的超参数，下一层的参数

![image-20240725103353513](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407251033636.png)



假设我把输入和输出的高和宽都减半的情况下，通常我就会把输出的通道数加一倍





池化操作

![image-20240725165205490](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407251659523.png)



池化层的作用

![image-20240725165829797](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407251659232.png)



常见的卷积网络中每个小部分的顺序是：卷积，激活，池化



多通道输入，最后每一次计算完卷积操作之后，会有一个相加的操作，也就是说即使是3通道的RGB的图片，经过卷积操作之后也是只有一个维度的

![image-20240725213335910](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407252133987.png)



多通道输出，感觉就类似于全连接层一样，每一层就增加很多个权重共同进行预测。

![image-20240725213655095](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407252136150.png)



LeNet应该是第一个神经网络

![image-20240726092545341](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407260925405.png)



---

alexNet和LeNet的比较

![image-20240726105657967](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407261056158.png)



alexNet总结

![image-20240726142719318](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407261427553.png)



---

VGG

![image-20240726163143754](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407261637769.png)



GoogleNet设计

![image-20240726192124854](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407261921223.png)

整体网络架构

![image-20240726193628295](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407261936431.png)





批量归一化

![image-20240726202810865](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407262028034.png)



![image-20240726204212283](https://fastly.jsdelivr.net/gh/unishuai/PicGoImg@main/limuStudy202407262042420.png)